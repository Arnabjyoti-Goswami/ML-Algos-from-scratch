{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RVG3GV8V6FqG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Set random seed for reproducibility of results, used in numpy.random and sklearn's train-test split\n",
        "rs = 42 \n",
        "np.random.seed(rs) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lShMhHXCCQ4e"
      },
      "source": [
        "# Implement the DecisionTree algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3EDfv_ae9NzP"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "  def __init__(self, feature=None, threshold=None, left_node=None, right_node=None, *,value=None):\n",
        "    self.feature = feature\n",
        "    self.threshold = threshold\n",
        "    self.left_node = left_node\n",
        "    self.right_node = right_node\n",
        "    self.value = value\n",
        "\n",
        "  def is_leaf_node(self):\n",
        "    return self.value is not None\n",
        "  \n",
        "def most_common_element(iterable):\n",
        "  unique_elements, counts = np.unique(iterable, return_counts=True)\n",
        "  return unique_elements[np.argmax(counts)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "teZcz7osCP49"
      },
      "outputs": [],
      "source": [
        "class DecisionTree:\n",
        "  def __init__(self, min_examples_for_split=2, max_depth=100, num_features=None):\n",
        "    self.min_examples_for_split = min_examples_for_split\n",
        "    self.max_depth = max_depth\n",
        "    self.num_features = num_features\n",
        "    self.root = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    # If only a subset of the total number features is to be considered (useful later for implementing Random Forests),\n",
        "    # then select specified number of features randomly, else consider all the features present in the dataset\n",
        "    self.num_features = X.shape[1] if not self.num_features else min(X.shape[1], self.num_features)\n",
        "    self.root = self._grow_tree(X, y)\n",
        "\n",
        "  def _grow_tree(self, X, y, depth=0):\n",
        "    num_examples, num_features = X.shape\n",
        "    num_labels = len(np.unique(y))\n",
        "\n",
        "    # Check the criteria for stopping the growth of the tree\n",
        "    if (depth >= self.max_depth or num_labels == 1 or num_examples < self.min_examples_for_split):\n",
        "      return Node(value = self._most_common_label(y))\n",
        "\n",
        "    # Choose the specified number of features randomly\n",
        "    feature_indices = np.random.choice(num_features, self.num_features, replace=False)\n",
        "\n",
        "    # Find the best feature and the best threshold value for the split on this node\n",
        "    best_feature_index, best_threshold = self._best_split(X, y, feature_indices)\n",
        "\n",
        "    # Create child left and right nodes and recursively call this function to grow the tree\n",
        "    left_indices, right_indices = self._split(X[:, best_feature_index], best_threshold)\n",
        "    left_node = self._grow_tree(X[left_indices, :], y[left_indices], depth+1)\n",
        "    right_node = self._grow_tree(X[right_indices, :], y[right_indices], depth+1)\n",
        "    return Node(best_feature_index, best_threshold, left_node, right_node)\n",
        "\n",
        "  def _best_split(self, X, y, feature_indices):\n",
        "    best_gain = -1\n",
        "    best_feature_index, best_threshold = None, None\n",
        "    # threshold is the value of the best feature, based on which the node is split into right and left nodes\n",
        "    # it's one of the many unique values present in the best feature's column\n",
        "\n",
        "    for feature_index in feature_indices:\n",
        "      X_column = X[:, feature_index]\n",
        "      thresholds = np.unique(X_column)\n",
        "\n",
        "      for threshold in thresholds:\n",
        "        # calculate the information gain\n",
        "        gain = self._information_gain(y, X_column, threshold)\n",
        "\n",
        "        # if a new maximum for the best gain is obtained then update the best gain, and the feature index and the threshold to split on.\n",
        "        if gain > best_gain:\n",
        "          best_gain = gain\n",
        "          best_feature_index = feature_index\n",
        "          best_threshold = threshold\n",
        "\n",
        "    return best_feature_index, best_threshold\n",
        "\n",
        "  def _split(self, X_column, split_threshold):\n",
        "    left_indices = np.argwhere(X_column <= split_threshold).flatten()\n",
        "    right_indices = np.argwhere(X_column > split_threshold).flatten()\n",
        "    return left_indices, right_indices\n",
        "\n",
        "  def _most_common_label(self, y):\n",
        "    value = most_common_element(y)\n",
        "    return value # most common label present in a leaf node\n",
        "\n",
        "  def _entropy(self, y):\n",
        "    hist = np.bincount(y) # This is an array where each element at index i represents the count of occurrences\n",
        "    # of the integer i in the input array. Here i is from 0 to the maximum positive integer present in the input array.\n",
        "\n",
        "    p_s = hist / len(y) # array of all the p's, where p[i] = (hist[i] / np.sum(hist))\n",
        "    return -np.sum([p * np.log(p) for p in p_s if p>0]) # p must be > 0 else log(p) is undefined\n",
        "\n",
        "  def _information_gain(self, y, X_column, threshold):\n",
        "    parent_entropy = self._entropy(y)\n",
        "\n",
        "    # Create child nodes and calculate their weighted entropy\n",
        "    left_indices, right_indices = self._split(X_column, threshold)\n",
        "    if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "      return 0\n",
        "    entropy_from_left_child, entropy_from_right_child = self._entropy(y[left_indices]), self._entropy(y[right_indices])\n",
        "    child_entropy = (len(left_indices)/len(y)) * entropy_from_left_child + (len(right_indices)/len(y)) * entropy_from_right_child\n",
        "\n",
        "    # Calculate and return the information gain for this particular split\n",
        "    information_gain = parent_entropy - child_entropy\n",
        "    return information_gain\n",
        "\n",
        "  def predict(self, X):\n",
        "    return np.array([self._traverse_tree(x, self.root) for x in X]) # make a label prediction by traversing the tree for every example x present in X\n",
        "\n",
        "  def _traverse_tree(self, x, node):\n",
        "    if node.is_leaf_node():\n",
        "      return node.value\n",
        "\n",
        "    # Recursively call this function to traverse the tree\n",
        "    if x[node.feature] <= node.threshold:\n",
        "      return self._traverse_tree(x, node.left_node)\n",
        "    else:\n",
        "      return self._traverse_tree(x, node.right_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVHbYydQLjTo"
      },
      "source": [
        "# Test the model on a Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2_5bFx1ILnrO"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = datasets.load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rs)\n",
        "\n",
        "model = DecisionTree(max_depth=10)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oC21ZLEoMQ9S"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_test, y_pred):\n",
        "  acc = np.sum(y_test == y_pred) / len(y_test)\n",
        "  acc *= 100\n",
        "  acc = round(acc, 2)\n",
        "  return acc\n",
        "\n",
        "def calculate_metrics(y_test, y_pred):\n",
        "  true_positives = np.sum(np.logical_and(y_test == 1, y_pred == 1))\n",
        "  false_positives = np.sum(np.logical_and(y_test == 0, y_pred == 1))\n",
        "  false_negatives = np.sum(np.logical_and(y_test == 1, y_pred == 0))\n",
        "  precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
        "  recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
        "  f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "  confusion_matrix = np.array([[true_positives, false_positives], [false_negatives, len(y_test) - true_positives]])\n",
        "  return precision, recall, f1_score, confusion_matrix\n",
        "\n",
        "def print_confusion_matrix(conf_matrix):\n",
        "  true_positives, false_positives, false_negatives, true_negatives = conf_matrix.ravel()\n",
        "  print(f\"                    Actual Positive    | Actual Negative\")\n",
        "  print(f\"Predicted Positive |       {true_positives} (TP)     |    {false_positives} (FP)\")\n",
        "  print(f\"Predicted Negative |       {false_negatives} (FN)     |    {true_negatives} (TN)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBVAmnqlM-v1",
        "outputId": "bd707349-9f4e-4ab1-fff0-b7a828139d4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 93.86%\n",
            "Precision: 0.9444444444444444\n",
            "Recall: 0.9577464788732394\n",
            "F1 Score: 0.951048951048951\n",
            "Confusion Matrix:\n",
            "                    Actual Positive    | Actual Negative\n",
            "Predicted Positive |       68 (TP)     |    4 (FP)\n",
            "Predicted Negative |       3 (FN)     |    46 (TN)\n"
          ]
        }
      ],
      "source": [
        "acc = accuracy(y_test, y_pred)\n",
        "precision, recall, f1_score, confusion_matrix = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {acc}%\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1_score)\n",
        "print(\"Confusion Matrix:\")\n",
        "print_confusion_matrix(confusion_matrix)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
